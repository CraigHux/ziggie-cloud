{
  "name": "Ziggie Knowledge Base QA",
  "description": "RAG pipeline for querying the Ziggie knowledge base (185+ markdown files). Uses Ollama LLM with in-memory vector store.",
  "nodes": [
    {
      "id": "directoryLoader_0",
      "position": { "x": 100, "y": 200 },
      "type": "customNode",
      "data": {
        "id": "directoryLoader_0",
        "label": "Directory Loader",
        "name": "directoryLoader",
        "category": "Document Loaders",
        "description": "Load markdown files from knowledge base directory",
        "inputParams": [
          {
            "name": "folderPath",
            "type": "string",
            "label": "Folder Path",
            "description": "Path to the knowledge base folder",
            "value": "/app/data/knowledge-base"
          },
          {
            "name": "recursive",
            "type": "boolean",
            "label": "Recursive",
            "description": "Recursively load files from subdirectories",
            "value": true
          },
          {
            "name": "extensions",
            "type": "string",
            "label": "File Extensions",
            "description": "Comma-separated list of file extensions to load",
            "value": ".md,.txt"
          }
        ],
        "inputAnchors": [],
        "outputs": {
          "output": "documents"
        },
        "outputAnchors": [
          {
            "id": "directoryLoader_0-output-output-documents",
            "name": "output",
            "label": "Documents",
            "type": "documents"
          }
        ]
      }
    },
    {
      "id": "markdownSplitter_0",
      "position": { "x": 400, "y": 200 },
      "type": "customNode",
      "data": {
        "id": "markdownSplitter_0",
        "label": "Markdown Text Splitter",
        "name": "markdownTextSplitter",
        "category": "Text Splitters",
        "description": "Split markdown documents by headers and sections",
        "inputParams": [
          {
            "name": "chunkSize",
            "type": "number",
            "label": "Chunk Size",
            "description": "Maximum size of each chunk",
            "value": 1500
          },
          {
            "name": "chunkOverlap",
            "type": "number",
            "label": "Chunk Overlap",
            "description": "Overlap between chunks",
            "value": 200
          }
        ],
        "inputAnchors": [
          {
            "id": "markdownSplitter_0-input-document-documents",
            "name": "document",
            "label": "Documents",
            "type": "documents"
          }
        ],
        "outputs": {
          "output": "documents"
        },
        "outputAnchors": [
          {
            "id": "markdownSplitter_0-output-output-documents",
            "name": "output",
            "label": "Documents",
            "type": "documents"
          }
        ]
      }
    },
    {
      "id": "ollamaEmbedding_0",
      "position": { "x": 400, "y": 400 },
      "type": "customNode",
      "data": {
        "id": "ollamaEmbedding_0",
        "label": "Ollama Embeddings",
        "name": "ollamaEmbedding",
        "category": "Embeddings",
        "description": "Generate embeddings using Ollama (nomic-embed-text)",
        "inputParams": [
          {
            "name": "baseUrl",
            "type": "string",
            "label": "Base URL",
            "description": "Ollama server URL",
            "value": "http://ollama:11434"
          },
          {
            "name": "modelName",
            "type": "string",
            "label": "Model Name",
            "description": "Embedding model to use",
            "value": "nomic-embed-text"
          }
        ],
        "inputAnchors": [],
        "outputs": {
          "output": "embeddings"
        },
        "outputAnchors": [
          {
            "id": "ollamaEmbedding_0-output-output-embeddings",
            "name": "output",
            "label": "Embeddings",
            "type": "embeddings"
          }
        ]
      }
    },
    {
      "id": "memoryVectorStore_0",
      "position": { "x": 700, "y": 300 },
      "type": "customNode",
      "data": {
        "id": "memoryVectorStore_0",
        "label": "In-Memory Vector Store",
        "name": "memoryVectorStore",
        "category": "Vector Stores",
        "description": "Store document embeddings in memory for fast retrieval",
        "inputParams": [
          {
            "name": "topK",
            "type": "number",
            "label": "Top K",
            "description": "Number of top results to return",
            "value": 6
          }
        ],
        "inputAnchors": [
          {
            "id": "memoryVectorStore_0-input-document-documents",
            "name": "document",
            "label": "Documents",
            "type": "documents"
          },
          {
            "id": "memoryVectorStore_0-input-embeddings-embeddings",
            "name": "embeddings",
            "label": "Embeddings",
            "type": "embeddings"
          }
        ],
        "outputs": {
          "output": "vectorStore",
          "retriever": "retriever"
        },
        "outputAnchors": [
          {
            "id": "memoryVectorStore_0-output-retriever-retriever",
            "name": "retriever",
            "label": "Retriever",
            "type": "retriever"
          }
        ]
      }
    },
    {
      "id": "ollamaChat_0",
      "position": { "x": 700, "y": 500 },
      "type": "customNode",
      "data": {
        "id": "ollamaChat_0",
        "label": "Ollama Chat Model",
        "name": "chatOllama",
        "category": "Chat Models",
        "description": "Ollama LLM for generating responses",
        "inputParams": [
          {
            "name": "baseUrl",
            "type": "string",
            "label": "Base URL",
            "description": "Ollama server URL",
            "value": "http://ollama:11434"
          },
          {
            "name": "modelName",
            "type": "string",
            "label": "Model Name",
            "description": "Chat model to use",
            "value": "llama3.2"
          },
          {
            "name": "temperature",
            "type": "number",
            "label": "Temperature",
            "description": "Sampling temperature (0-1)",
            "value": 0.3
          }
        ],
        "inputAnchors": [],
        "outputs": {
          "output": "chatModel"
        },
        "outputAnchors": [
          {
            "id": "ollamaChat_0-output-output-chatModel",
            "name": "output",
            "label": "Chat Model",
            "type": "chatModel"
          }
        ]
      }
    },
    {
      "id": "conversationalRetrievalQA_0",
      "position": { "x": 1000, "y": 350 },
      "type": "customNode",
      "data": {
        "id": "conversationalRetrievalQA_0",
        "label": "Conversational Retrieval QA Chain",
        "name": "conversationalRetrievalQAChain",
        "category": "Chains",
        "description": "QA chain with conversation memory and retrieval",
        "inputParams": [
          {
            "name": "systemMessagePrompt",
            "type": "string",
            "label": "System Message",
            "description": "System prompt for the assistant",
            "value": "You are Ziggie, an AI assistant for the MeowPing RTS game development project. You have access to the project's knowledge base which contains documentation about:\n\n- Game development best practices (RTS, UI/UX, multiplayer)\n- AI asset generation pipelines (ComfyUI, Blender, 3D-to-2D rendering)\n- Elite AI agent team structure (15 specialized agents)\n- Project management and quality assurance\n- AWS cloud integration and infrastructure\n\nWhen answering questions:\n1. Be specific and reference relevant documentation when possible\n2. Provide actionable guidance for game development tasks\n3. If you don't know something, say so rather than guessing\n4. Format responses clearly with headers and bullet points when appropriate"
          },
          {
            "name": "returnSourceDocuments",
            "type": "boolean",
            "label": "Return Source Documents",
            "description": "Return the source documents used for the answer",
            "value": true
          }
        ],
        "inputAnchors": [
          {
            "id": "conversationalRetrievalQA_0-input-model-chatModel",
            "name": "model",
            "label": "Chat Model",
            "type": "chatModel"
          },
          {
            "id": "conversationalRetrievalQA_0-input-vectorStoreRetriever-retriever",
            "name": "vectorStoreRetriever",
            "label": "Vector Store Retriever",
            "type": "retriever"
          }
        ],
        "outputs": {
          "output": "response"
        },
        "outputAnchors": [
          {
            "id": "conversationalRetrievalQA_0-output-output-response",
            "name": "output",
            "label": "Response",
            "type": "response"
          }
        ]
      }
    },
    {
      "id": "bufferMemory_0",
      "position": { "x": 1000, "y": 550 },
      "type": "customNode",
      "data": {
        "id": "bufferMemory_0",
        "label": "Buffer Memory",
        "name": "bufferMemory",
        "category": "Memory",
        "description": "Store conversation history in memory",
        "inputParams": [
          {
            "name": "memoryKey",
            "type": "string",
            "label": "Memory Key",
            "value": "chat_history"
          },
          {
            "name": "sessionId",
            "type": "string",
            "label": "Session ID",
            "value": ""
          }
        ],
        "inputAnchors": [],
        "outputs": {
          "output": "memory"
        },
        "outputAnchors": [
          {
            "id": "bufferMemory_0-output-output-memory",
            "name": "output",
            "label": "Memory",
            "type": "memory"
          }
        ]
      }
    }
  ],
  "edges": [
    {
      "id": "directoryLoader_0-markdownSplitter_0",
      "source": "directoryLoader_0",
      "sourceHandle": "directoryLoader_0-output-output-documents",
      "target": "markdownSplitter_0",
      "targetHandle": "markdownSplitter_0-input-document-documents",
      "type": "buttonedge"
    },
    {
      "id": "markdownSplitter_0-memoryVectorStore_0",
      "source": "markdownSplitter_0",
      "sourceHandle": "markdownSplitter_0-output-output-documents",
      "target": "memoryVectorStore_0",
      "targetHandle": "memoryVectorStore_0-input-document-documents",
      "type": "buttonedge"
    },
    {
      "id": "ollamaEmbedding_0-memoryVectorStore_0",
      "source": "ollamaEmbedding_0",
      "sourceHandle": "ollamaEmbedding_0-output-output-embeddings",
      "target": "memoryVectorStore_0",
      "targetHandle": "memoryVectorStore_0-input-embeddings-embeddings",
      "type": "buttonedge"
    },
    {
      "id": "memoryVectorStore_0-conversationalRetrievalQA_0",
      "source": "memoryVectorStore_0",
      "sourceHandle": "memoryVectorStore_0-output-retriever-retriever",
      "target": "conversationalRetrievalQA_0",
      "targetHandle": "conversationalRetrievalQA_0-input-vectorStoreRetriever-retriever",
      "type": "buttonedge"
    },
    {
      "id": "ollamaChat_0-conversationalRetrievalQA_0",
      "source": "ollamaChat_0",
      "sourceHandle": "ollamaChat_0-output-output-chatModel",
      "target": "conversationalRetrievalQA_0",
      "targetHandle": "conversationalRetrievalQA_0-input-model-chatModel",
      "type": "buttonedge"
    }
  ],
  "metadata": {
    "version": "1.0.0",
    "created": "2025-12-27",
    "author": "Ziggie AI Pipeline Agent",
    "requirements": {
      "ollama_models": ["llama3.2", "nomic-embed-text"],
      "data_path": "/app/data/knowledge-base",
      "estimated_documents": 185,
      "chunk_size": 1500,
      "chunk_overlap": 200
    },
    "notes": [
      "Mount knowledge base to /app/data/knowledge-base in Docker",
      "Ensure Ollama has nomic-embed-text model for embeddings",
      "Ensure Ollama has llama3.2 model for chat",
      "In-memory vector store resets on restart - consider Pinecone for persistence"
    ]
  }
}
